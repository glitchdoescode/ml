# Value Iteration vs. Policy Iteration: Solving MDPs (PYQ 6b - 2024, PYQ 7b - CBGS)\n\n## 1. Introduction: Finding the Optimal Policy in MDPs\n\nValue Iteration (VI) and Policy Iteration (PI) are two classical **dynamic programming** algorithms used to solve a known Markov Decision Process (MDP). This means they are **model-based** algorithms – they require complete knowledge of the environment\'s dynamics: the transition probabilities `P(s\'|s,a)` and the reward function `R(s,a,s\')` (or `R(s)` or `R(s,a)`).\n\nThe ultimate goal of both VI and PI is to find the **optimal policy (`π*`)**, which dictates the best action to take in each state to maximize the expected cumulative discounted reward.\n\n*   **Dynamic Programming (DP):** These methods break down the complex problem of finding an optimal policy into a sequence of simpler subproblems. They use the Bellman equations to iteratively compute value functions.\n\n## 2. Value Iteration (VI)\n\n**Core Idea:** Value Iteration focuses on directly finding the **optimal state-value function (`V*(s)`)** by iteratively applying the Bellman Optimality Equation. Once `V*(s)` is found (or closely approximated), the optimal policy `π*(s)` can be extracted from it.\n\n**Algorithm Steps:**\n\n1.  **Initialization:**\n    *   Initialize the state-value function `V_0(s)` to arbitrary values for all states `s ∈ S` (e.g., `V_0(s) = 0` for all non-terminal states). `V(terminal\_state) = 0` always.\n    *   Choose a small positive threshold `θ` (theta) for checking convergence.\n\n2.  **Iteration Loop:**\n    *   Repeat for `k = 0, 1, 2, ...`:\n        *   Set `Δ = 0` (delta, to track the maximum change in V(s) in this iteration).\n        *   For each state `s ∈ S`:\n            *   Store the old value: `v = V_k(s)`.\n            *   Update the value of state `s` using the Bellman Optimality Equation:\n                `V_{k+1}(s) ← max_a Σ_{s\'∈S} P(s\'|s,a) [R(s,a,s\') + γV_k(s\')]`\n                (Where `P(s\'|s,a)` is the transition probability to state `s\'` given state `s` and action `a`, `R(s,a,s\')` is the reward, and `γ` is the discount factor.)\n            *   Update `Δ = max(Δ, |v - V_{k+1}(s)|)`.\n        *   If `Δ < θ`, then break the loop (the value function has converged sufficiently).\n\n3.  **Optimal Policy Extraction:**\n    *   Once the value function `V(s)` has converged to `V*(s)`, the optimal policy `π*(s)` is determined by choosing the action that maximizes the expected return from each state `s`:\n        `π*(s) = argmax_a Σ_{s\'∈S} P(s\'|s,a) [R(s,a,s\') + γV*(s\')]`\n\n**How it Works (Intuition):**\nValue Iteration can be thought of as finding the optimal value function for a problem with a horizon of 1 step, then 2 steps, and so on. In each iteration `k`, `V_k(s)` represents the optimal value achievable starting from state `s` if the agent is allowed to act for `k` more steps. As `k` approaches infinity, `V_k(s)` converges to `V*(s)`, the true optimal value function for an infinite horizon.\n\nThe update `V_{k+1}(s) ← max_a Q_k(s,a)` where `Q_k(s,a) = Σ_{s\'} P(s\'|s,a) [R(s,a,s\') + γV_k(s\')]` is essentially applying the Bellman backup.\n\n**Example (Conceptual):**\nImagine a simple grid. `V_0` might be all zeros. `V_1` will assign non-zero values only to states that can reach a rewarding terminal state in one step. `V_2` will propagate these values further back, considering states that can reach those `V_1` states, and so on, until the values stabilize across the grid.\n\n## 3. Policy Iteration (PI)\n\n**Core Idea:** Policy Iteration alternates between two main steps until the policy no longer improves:\n1.  **Policy Evaluation:** Given the current policy `π`, calculate its state-value function `V^π(s)`.\n2.  **Policy Improvement:** Using `V^π(s)`, find a new (and potentially better) policy `π\'` by acting greedily with respect to `V^π(s)`.\n\n**Algorithm Steps:**\n\n1.  **Initialization:**\n    *   Initialize policy `π_0(s)` arbitrarily for all states `s ∈ S`.\n    *   Initialize `V^π(s)` arbitrarily (e.g., `V^π(s) = 0`) or use the value from a previous policy evaluation if available.\n\n2.  **Iteration Loop:** Repeat until the policy `π` is stable:\n    a.  **Policy Evaluation (Iterative Approach):**\n        *   Given the current policy `π_k`, we want to compute `V^{π_k}(s)`.\n        *   This means solving the system of linear equations defined by the Bellman equation for policy `π_k`:\n            `V^{π_k}(s) = Σ_{s\'∈S} P(s\'|s, π_k(s)) [R(s, π_k(s), s\') + γV^{π_k}(s\')]`\n            (Note: `π_k(s)` is the specific action taken by the current policy in state `s`)\n        *   This system can be solved iteratively (similar to Value Iteration, but without the `max_a`): For a fixed `π_k`, initialize `V_0^{π_k}(s)` and loop:\n            `V_{i+1}^{π_k}(s) ← Σ_{s\'∈S} P(s\'|s, π_k(s)) [R(s, π_k(s), s\') + γV_i^{π_k}(s\')]`\n            until `V_i^{π_k}(s)` converges to `V^{π_k}(s)`. (Requires a convergence check with a threshold `θ`.)\n\n    b.  **Policy Improvement:**\n        *   Set `policy_stable = true`.\n        *   For each state `s ∈ S`:\n            *   Store the old action: `old_action = π_k(s)`.\n            *   Find the new best action by acting greedily with respect to `V^{π_k}(s)`:\n                `π_{k+1}(s) ← argmax_a Σ_{s\'∈S} P(s\'|s,a) [R(s,a,s\') + γV^{π_k}(s\')]`\n            *   If `old_action ≠ π_{k+1}(s)`, then set `policy_stable = false`.\n\n    c.  **Check for Stability:**\n        *   If `policy_stable` is true, then `π_{k+1}` is the optimal policy `π*` and `V^{π_k}` is `V*`. Break the loop.\n        *   Otherwise, set `π_k ← π_{k+1}` (update the policy) and go back to step 2a (Policy Evaluation).\n\n**How it Works (Intuition):**\nPI starts with some policy and finds out how good it is (evaluation). Then, it asks: \"Knowing these values, can I pick actions in a greedier way to make my policy even better?\" (improvement). It repeats this until it can\'t find any better greedy actions, meaning the policy is optimal.\nEach policy generated by PI is guaranteed to be a strict improvement over the previous one (unless it\'s already optimal). Since there are a finite number of policies in a finite MDP, PI is guaranteed to converge.\n\n## 4. Value Iteration vs. Policy Iteration: Head-to-Head Comparison\n\n| Feature                 | Value Iteration (VI)                                    | Policy Iteration (PI)                                       |\n|-------------------------|---------------------------------------------------------|-------------------------------------------------------------|\n| **Primary Goal**        | Find optimal state-value function `V*(s)` first.       | Find optimal policy `π*(s)` directly.                       |\n| **Core Operation**      | Iteratively apply Bellman Optimality Equation.         | Alternate Policy Evaluation & Policy Improvement.           |\n| **Policy in Loop**      | No explicit policy until the end.                       | Maintains and improves an explicit policy in each iteration. |\n| **Computational Cost per Main Iteration** | One sweep through states, applying `max_a` over actions.  | **Policy Evaluation:** Can be multiple sweeps (iterative solution of linear equations) for the current policy. <br/> **Policy Improvement:** One sweep through states, `argmax_a` over actions. |\n| **Iterations to Converge** | Often more iterations than PI.                          | Often fewer *policy improvement* iterations than VI.           |\n| **Overall Complexity**  | Simpler to implement one VI iteration. Each iteration is generally less complex than one full PI iteration (which includes full policy evaluation). | One PI iteration can be more complex if policy evaluation is iterative and slow. |\n| **Convergence Guarantee** | Guaranteed to converge to `V*(s)`.                     | Guaranteed to converge to `π*(s)` and `V*(s)`.                |\n| **When it stops**       | When `V(s)` values stabilize (`Δ < θ`).                 | When the policy `π(s)` no longer changes.                   |\n\n## 5. When to Use Which?\n\n*   **Policy Iteration (PI):**\n    *   Often converges in fewer iterations (of policy changes).\n    *   Can be faster if the policy evaluation step can be done very efficiently (e.g., if the number of states is small enough to solve the system of equations directly, or if the iterative evaluation converges quickly).\n    *   Generally preferred when the action space is small or the number of possible distinct policies is not excessively large.\n\n*   **Value Iteration (VI):**\n    *   Simpler to implement each individual iteration.\n    *   May be computationally cheaper per iteration if the policy evaluation step in PI is very slow (e.g., many iterations needed for `V^π` to converge).\n    *   Can be more efficient when the number of states is very large and the cost of a full policy evaluation sweep is high.\n    *   Sometimes, a few iterations of VI can give a reasonably good policy quickly, even if not fully converged.\n\n**Truncated Policy Iteration:**\nIn practice, a hybrid approach is often used. For example, in Policy Iteration, the Policy Evaluation step doesn\'t have to be run until `V^π` fully converges. A few sweeps of updates towards `V^π` can be enough before moving to Policy Improvement. This is a form of Generalized Policy Iteration (GPI).\n\n## 6. Generalized Policy Iteration (GPI)\n\nGPI refers to the general idea of letting policy evaluation and policy improvement processes interact. Both VI and PI are specific instances of GPI.\nThe core idea is that an agent maintains an approximate value function and an approximate policy. The value function is updated to better match the current policy (evaluation), and the policy is updated to be greedy with respect to the current value function (improvement). This interleaving of evaluation and improvement steps, if done correctly, drives both the policy and value function towards optimality.\n\n## 7. Simple Grid World Example\n\nConsider a simple 2-state MDP:\n*   States: S1, S2 (S2 is terminal, reward +10 entering S2)\n*   Actions: a1, a2\n*   From S1, action a1 → S1 (reward -1), action a2 → S2 (reward 0 before terminal reward)\n*   `γ = 0.9`\n\n**Value Iteration (1st iteration, `V_0(S1)=0, V_0(S2)=0`):**\n`V_1(S1) = max {`\n  ` (1.0 * [-1 + 0.9 * V_0(S1)]), ` // Action a1: P(S1|S1,a1)=1\n  ` (1.0 * [0 + 0.9 * V_0(S2)]) `   // Action a2: P(S2|S1,a2)=1, but S2 is terminal, R(S2)=10 so this would be 10 when applying R(s,a,s\') if defined that way. Let\'s use R(s,a,s\') for clarity: R(S1,a2,S2)=10\n`}`\nIf R(s,a,s\') is used where `R(S1,a2,S2)=10`:\n`V_1(S1) = max { [-1 + 0.9*0], [10 + 0.9*0] } = max{-1, 10} = 10`\nPolicy derived: `argmax_a` would choose a2.\n\n**Policy Iteration (Initial policy `π_0(S1) = a1`):**\n1.  **Policy Evaluation for `π_0`:**\n    `V^{π_0}(S1) = R(S1,a1,S1) + γV^{π_0}(S1)`\n    `V^{π_0}(S1) = -1 + 0.9 * V^{π_0}(S1)`\n    `0.1 * V^{π_0}(S1) = -1  => V^{π_0}(S1) = -10`\n2.  **Policy Improvement (using `V^{π_0}(S1) = -10`):**\n    New policy `π_1(S1) = argmax_a { Q(S1,a) }`\n    `Q(S1,a1) = R(S1,a1,S1) + γV^{π_0}(S1) = -1 + 0.9 * (-10) = -1 - 9 = -10`\n    `Q(S1,a2) = R(S1,a2,S2) + γV^{π_0}(S2) = 10 + 0.9 * 0 = 10`\n    Since `Q(S1,a2) > Q(S1,a1)`, `π_1(S1) = a2`.\n    Policy changed, so continue.\n\n(Next PI iteration would evaluate policy `π_1(S1)=a2`, find `V^{π_1}(S1)=10`, then policy improvement would confirm `π_2(S1)=a2`. Policy stable.)\n\n## 8. Summary for Exams (PYQ 6b - 2024, PYQ 7b - CBGS)\n\n*   **Goal:** Both find optimal policy `π*` in a known MDP.\n*   **Value Iteration (VI):**\n    *   Iteratively updates `V(s)` using Bellman Optimality: `V(s) ← max_a Σ P[R + γV(s\')]`.\n    *   Extracts policy at the end.\n    *   Each iteration is simpler but may require more iterations.\n*   **Policy Iteration (PI):**\n    *   Alternates: **Policy Evaluation** (calculates `V^π(s)` for current `π` by solving `V^π(s) = Σ P[R + γV^π(s\')]`) and **Policy Improvement** (updates `π(s) ← argmax_a Σ P[R + γV^π(s\')]`).\n    *   Often converges in fewer policy update iterations.\n    *   Policy Evaluation step can be computationally intensive.\n*   **Key Difference:** VI iterates on value functions directly towards `V*`. PI iterates between finding the exact value of the current policy and then improving that policy greedily.\n*   Both are forms of **Generalized Policy Iteration (GPI)**.\n*   Both require a **model of the environment** (transition probabilities and rewards). 